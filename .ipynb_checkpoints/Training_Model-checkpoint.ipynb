{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af46e366",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='308' max='308' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [308/308 43:12, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel, TrainingArguments, Trainer, pipeline, DataCollatorForLanguageModeling\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "\n",
    "# Ensure the save directory exists\n",
    "model_save_dir = \"./model\"\n",
    "os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "# Load and preprocess the dataset\n",
    "data_path = r\"C:\\Users\\aasth\\OneDrive\\Desktop\\LLM_Mental_Health_Support_Chatbot\\Dataset\\mental_health_conversational_dataset_train.csv\"\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "def preprocess_text(row):\n",
    "    parts = row.split(\"<<<ASSISTANT>>>:\")\n",
    "    question = parts[0].replace(\"<<<HUMAN>>>:\", \"\").strip().lower()\n",
    "    answer = parts[1].strip().lower() if len(parts) > 1 else \"\"\n",
    "    return f\"{question} {answer}\"\n",
    "\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'pad_token': tokenizer.eos_token})\n",
    "\n",
    "# Encode the data\n",
    "inputs = tokenizer(data['processed_text'].tolist(), truncation=True, max_length=512, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "# Adjusted Dataset preparation\n",
    "class MentalHealthDataset(Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "# Adjusted dataset initialization with the corrected inputs\n",
    "dataset = MentalHealthDataset(inputs)\n",
    "\n",
    "# Model preparation\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=model_save_dir,\n",
    "    num_train_epochs=4,\n",
    "    per_device_train_batch_size=2,  # Adjust based on your GPU/CPU memory\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=10,\n",
    "    save_steps=1000,\n",
    "    eval_steps=1000,  # Ensure this aligns with save_steps or as per your evaluation frequency requirement\n",
    "    evaluation_strategy=\"steps\",  # Align evaluation strategy with save strategy\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Model training\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(model_save_dir)\n",
    "\n",
    "# Evaluation function refined for clarity and usability\n",
    "def evaluate_advice(prompt):\n",
    "    chatbot = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "    response = chatbot(prompt, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2)[0]['generated_text']\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f220d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Specifying the model to use for sentiment analysis\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m sentiment_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcollect_feedback\u001b[39m():\n\u001b[0;32m      5\u001b[0m     feedback \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeedback on the advice: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "# Specifying the model to use for sentiment analysis\n",
    "sentiment_model = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "def collect_feedback():\n",
    "    feedback = input(\"Feedback on the advice: \")\n",
    "    return feedback\n",
    "\n",
    "def chat_with_sentiment_analysis():\n",
    "    print(\"Welcome to the Mental Health Support Chatbot. Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        sentiment_result = sentiment_model(user_input)\n",
    "        sentiment = sentiment_result[0]['label']\n",
    "        if sentiment == 'NEGATIVE':\n",
    "            print(\"It sounds like you're going through a tough time. Let's see if I can help.\")\n",
    "        else:\n",
    "            print(\"That's good to hear! How can I assist you today?\")\n",
    "        response = evaluate_advice(user_input)\n",
    "        print(f\"Assistant: {response}\\n\")\n",
    "        feedback = collect_feedback()\n",
    "        print(f\"Thank you for your feedback: {feedback}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9215124",
   "metadata": {},
   "source": [
    "if __name__ == \"__main__\":\n",
    "    chat_with_sentiment_analysis()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d15d55a",
   "metadata": {},
   "source": [
    "# emotional response for currect training model if necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "157002af",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pipeline' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m chatbot\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Initialize Sentiment Analysis Pipeline\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m sentiment_model \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msentiment-analysis\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#Interactive Chat Function with Sentiment Analysis and Feedback Collection\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_with_sentiment_analysis\u001b[39m(chatbot):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pipeline' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Part 5: Loading Model and Tokenizer for Interaction\n",
    "def load_model_and_tokenizer(model_dir):\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_dir)\n",
    "    tokenizer = GPT2Tokenizer.from_pretrained(model_dir)\n",
    "    chatbot = pipeline('text-generation', model=model, tokenizer=tokenizer)\n",
    "    return chatbot\n",
    "\n",
    "# Initialize Sentiment Analysis Pipeline\n",
    "sentiment_model = pipeline('sentiment-analysis', model='distilbert-base-uncased-finetuned-sst-2-english')\n",
    "\n",
    "#Interactive Chat Function with Sentiment Analysis and Feedback Collection\n",
    "def chat_with_sentiment_analysis(chatbot):\n",
    "    print(\"Welcome to the Mental Health Support Chatbot. Type 'quit' to exit.\")\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'quit':\n",
    "            break\n",
    "        \n",
    "        # Analyze the sentiment of the input\n",
    "        sentiment_result = sentiment_model(user_input)\n",
    "        sentiment = sentiment_result[0]['label']\n",
    "        \n",
    "        # Tailor the response based on the sentiment\n",
    "        if sentiment == 'NEGATIVE':\n",
    "            print(\"It sounds like you're going through a tough time. Let's see if I can help.\")\n",
    "        else:\n",
    "            print(\"That's good to hear! How can I assist you today?\")\n",
    "        \n",
    "        # Generate the response from the chatbot\n",
    "        generated_responses = chatbot(user_input, max_length=150, num_return_sequences=1, no_repeat_ngram_size=2)\n",
    "        response = generated_responses[0]['generated_text']\n",
    "        response_trimmed = trim_to_last_sentence(response)\n",
    "        print(f\"Assistant: {response_trimmed}\\n\")\n",
    "        \n",
    "        # Collect user feedback on the response\n",
    "        feedback = input(\"Feedback on the advice: \")\n",
    "        sentiment_result = sentiment_model(feedback)\n",
    "        sentiment_feedback = sentiment_result[0]['label']\n",
    "        print(f\"Feedback sentiment: {sentiment_feedback}\")\n",
    "\n",
    "# Part 6: Start the Chat Session with Sentiment Analysis\n",
    "if __name__ == \"__main__\":\n",
    "    model_dir = \"./model\"  # Ensure this points to the directory where your model is saved\n",
    "    chatbot = load_model_and_tokenizer(model_dir)\n",
    "    chat_with_sentiment_analysis(chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150df9c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
